{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# force GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.src.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.src.layers import SimpleRNN, LSTM, Dense, Dropout\n",
    "from keras import Sequential, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 sequences\n",
      "Loaded 100000 sequences\n",
      "Loaded 150000 sequences\n",
      "Loaded 200000 sequences\n",
      "Loaded 250000 sequences\n",
      "Loaded 300000 sequences\n",
      "Loaded 350000 sequences\n",
      "Loaded 400000 sequences\n",
      "Loaded 447415 sequences\n",
      "Found 1988 sequences longer than 1000\n",
      "Split them into 18540 sequences\n",
      "Filtered out datapoints with less than 10 occurrences\n",
      "Left with 447415 datapoints\n"
     ]
    }
   ],
   "source": [
    "data_directory = '/home/jrosendahl/datasets/cadets/sequences/'\n",
    "\n",
    "data = []\n",
    "\n",
    "labels = None\n",
    "no_labels = None\n",
    "\n",
    "distinct_features = [\n",
    "'EVENT_ACCEPT', 'EVENT_BIND', 'EVENT_CHANGE_PRINCIPAL', 'EVENT_CLOSE', 'EVENT_CONNECT', 'EVENT_CREATE_OBJECT', 'EVENT_EXECUTE', 'EVENT_EXIT', 'EVENT_FCNTL', 'EVENT_FORK', 'EVENT_LINK', 'EVENT_LOGIN', 'EVENT_LSEEK', 'EVENT_MMAP', 'EVENT_MODIFY_FILE_ATTRIBUTES', 'EVENT_MODIFY_PROCESS', 'EVENT_MPROTECT', 'EVENT_OPEN', 'EVENT_OTHER', 'EVENT_READ', 'EVENT_RECVFROM', 'EVENT_RECVMSG', 'EVENT_RENAME', 'EVENT_SENDMSG', 'EVENT_SENDTO', 'EVENT_SIGNAL', 'EVENT_TRUNCATE', 'EVENT_UNLINK', 'EVENT_WRITE', \n",
    "]\n",
    "distinct_features = [ x[6:] for x in distinct_features ]\n",
    "no_features = len(distinct_features)\n",
    "\n",
    "count = 0\n",
    "count_long_sequences_splitted = 0\n",
    "count_long_sequences_splitted_result = 0\n",
    "for file_name in os.listdir(data_directory):\n",
    "    count += 1\n",
    "    if count % 50000 == 0:\n",
    "        print(f'Loaded {count} sequences')\n",
    "    with open(data_directory + file_name, 'r') as f:\n",
    "        label = file_name.split('_')[0]\n",
    "        # build dictionary with counts of events\n",
    "        lines = f.readlines()\n",
    "        # if sequence is longer than 1000, split into multiple sequences\n",
    "        if len(lines) > 1000:\n",
    "            count_long_sequences_splitted += 1\n",
    "            for i in range(0, len(lines), 1000):\n",
    "                count_long_sequences_splitted_result += 1\n",
    "                datapoints = []\n",
    "                for event in lines[i:i+1000]:\n",
    "                    event = event.strip()\n",
    "                    datapoints.append(distinct_features.index(event))\n",
    "                data.append((label, datapoints))\n",
    "        else:\n",
    "            datapoints = []\n",
    "            for event in lines:\n",
    "                event = event.strip()\n",
    "                datapoints.append(distinct_features.index(event))\n",
    "            data.append((label, datapoints))\n",
    "\n",
    "        \"\"\"\n",
    "        datapoints = []\n",
    "        for event in f:\n",
    "            event = event.strip()\n",
    "            datapoints.append(distinct_features.index(event))\n",
    "        if not len(datapoints) > 1000:\n",
    "            data.append((label, datapoints))\n",
    "        \"\"\"\n",
    "\n",
    "print(f'Loaded {len(data)} sequences')\n",
    "print(f'Found {count_long_sequences_splitted} sequences longer than 1000')\n",
    "print(f'Split them into {count_long_sequences_splitted_result} sequences')\n",
    "\n",
    "print(f'Filtered out datapoints with less than 10 occurrences')\n",
    "print(f'Left with {len(data)} datapoints')\n",
    "\n",
    "random.shuffle(data)\n",
    "labels = [ x[0] for x in data ]\n",
    "data = [ x[1] for x in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 135 distinct labels\n",
      "Found 29 distinct features\n",
      "Encoded labels to integers\n",
      "Encoded labels to one-hot\n",
      "Encoded sequences to one-hot\n"
     ]
    }
   ],
   "source": [
    "distinct_labels = np.unique(np.array(labels))\n",
    "no_labels = len(distinct_labels)\n",
    "print(f'Found {no_labels} distinct labels')\n",
    "\n",
    "# get distinct features\n",
    "print(f'Found {no_features} distinct features')\n",
    "\n",
    "# encode labels from strings to integers to one-hot\n",
    "# labels = np.eye(no_labels)[np.vectorize(distinct_labels.tolist().index)(labels)]\n",
    "# encode labels to integers\n",
    "labels = np.vectorize(distinct_labels.tolist().index)(labels)\n",
    "print(f'Encoded labels to integers')\n",
    "# encode labels to one-hot\n",
    "labels = np.eye(no_labels)[labels]\n",
    "print(f'Encoded labels to one-hot')\n",
    "\n",
    "# encode sequences from list of integers to list of one-hot\n",
    "data = [ np.eye(no_features)[x] for x in data ]\n",
    "print(f'Encoded sequences to one-hot')\n",
    "\n",
    "# get pad event to pad sequences with when batches are built\n",
    "pad_event = np.zeros(no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 447415\n",
      "labels shape: (447415, 135)\n"
     ]
    }
   ],
   "source": [
    "print(f'data length: {len(data)}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare data + generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 357932 samples\n",
      "Validating on 89483 samples\n"
     ]
    }
   ],
   "source": [
    "# split data into training and validation\n",
    "split = int(len(data) * 0.8)\n",
    "\n",
    "X_train = data[:split]\n",
    "y_train = np.array(labels[:split])\n",
    "\n",
    "X_val = data[split:]\n",
    "y_val = np.array(labels[split:])\n",
    "\n",
    "print(f'Training on {len(X_train)} samples')\n",
    "print(f'Validating on {len(X_val)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size, fixed_length, **kwargs):\n",
    "        # valid **kwargs: workers, use_multiprocessing, max_queue_size\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.fixed_length = fixed_length\n",
    "        self.no_samples = len(X)\n",
    "        self.no_batches = int(np.ceil(self.no_samples / self.batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.no_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = min(start + self.batch_size, self.no_samples)\n",
    "\n",
    "        # Get the batch data\n",
    "        X_batch = self.X[start:end]\n",
    "        y_batch = self.y[start:end]\n",
    "\n",
    "        # Pad or truncate each sequence in X_batch to the fixed length\n",
    "        X_batch_fixed = [self._pad_or_truncate(x, self.fixed_length) for x in X_batch]\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        X_batch_fixed = np.array(X_batch_fixed)\n",
    "        y_batch = np.array(y_batch)\n",
    "\n",
    "        return X_batch_fixed, y_batch\n",
    "\n",
    "    def _pad_or_truncate(self, sequence, length):\n",
    "        if len(sequence) < length:\n",
    "            # Pad sequence with zeros to the fixed length\n",
    "            return np.pad(sequence, ((0, length - len(sequence)), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            # Truncate sequence to the fixed length\n",
    "            return sequence[:length]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,016</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">135</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,775</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m6,016\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m135\u001b[0m)            │         \u001b[38;5;34m8,775\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,303</span> (122.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,303\u001b[0m (122.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,303</span> (122.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,303\u001b[0m (122.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential(layers=[\n",
    "    Input(shape=(None, no_features)),\n",
    "    SimpleRNN(64, return_sequences=True),\n",
    "    SimpleRNN(64, return_sequences=True),\n",
    "    SimpleRNN(64, return_sequences=False),\n",
    "    Dense(no_labels, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "checkpoint_path = (\"/home/jrosendahl/sync/models/checkpoints\")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f'{checkpoint_path}/rnn_simple.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.1, \n",
    "    patience=3, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    filename=f'{checkpoint_path}/rnn.log',\n",
    "    append=True\n",
    ")\n",
    "\n",
    "\n",
    "# 'categorical_focal_crossentropy'\n",
    "# 'categorical_crossentropy'\n",
    "model.compile(optimizer='adam', loss='categorical_focal_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718258609.110637  487352 service.cc:145] XLA service 0x867b760 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1718258609.110685  487352 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "I0000 00:00:1718258609.110690  487352 service.cc:153]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/5593\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:28:56\u001b[0m 4s/step - accuracy: 0.0000e+00 - loss: 1.2245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1718258610.536306  487352 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.3588 - loss: 0.5451\n",
      "Epoch 1: val_loss improved from inf to 0.53439, saving model to /home/jrosendahl/sync/models/checkpoints/rnn_simple.keras\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1232s\u001b[0m 220ms/step - accuracy: 0.3588 - loss: 0.5451 - val_accuracy: 0.3557 - val_loss: 0.5344 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.3603 - loss: 0.5284\n",
      "Epoch 2: val_loss improved from 0.53439 to 0.53325, saving model to /home/jrosendahl/sync/models/checkpoints/rnn_simple.keras\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1229s\u001b[0m 220ms/step - accuracy: 0.3603 - loss: 0.5284 - val_accuracy: 0.3557 - val_loss: 0.5333 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.3648 - loss: 0.5217\n",
      "Epoch 3: val_loss improved from 0.53325 to 0.26494, saving model to /home/jrosendahl/sync/models/checkpoints/rnn_simple.keras\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1230s\u001b[0m 220ms/step - accuracy: 0.3648 - loss: 0.5217 - val_accuracy: 0.6227 - val_loss: 0.2649 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.5595 - loss: 0.3320\n",
      "Epoch 4: val_loss did not improve from 0.26494\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1230s\u001b[0m 220ms/step - accuracy: 0.5595 - loss: 0.3320 - val_accuracy: 0.5657 - val_loss: 0.2886 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.5490 - loss: 0.3096\n",
      "Epoch 5: val_loss did not improve from 0.26494\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1229s\u001b[0m 220ms/step - accuracy: 0.5490 - loss: 0.3096 - val_accuracy: 0.4377 - val_loss: 0.4230 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.4434 - loss: 0.4192\n",
      "Epoch 6: val_loss did not improve from 0.26494\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1233s\u001b[0m 220ms/step - accuracy: 0.4434 - loss: 0.4192 - val_accuracy: 0.5385 - val_loss: 0.3513 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.5156 - loss: 0.3690\n",
      "Epoch 7: val_loss did not improve from 0.26494\n",
      "\u001b[1m5593/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1227s\u001b[0m 219ms/step - accuracy: 0.5156 - loss: 0.3690 - val_accuracy: 0.4655 - val_loss: 0.3758 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m4363/5593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m3:51\u001b[0m 188ms/step - accuracy: 0.4760 - loss: 0.3639"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=Generator(X_train, y_train, 64, 1000),\n",
    "    validation_data=Generator(X_val, y_val, 32, 1000),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, model_checkpoint, lr_schedule, csv_logger],\n",
    ")\n",
    "\n",
    "# save history to file\n",
    "with open(f'{checkpoint_path}/rnn_history.npy', 'wb') as f:\n",
    "    np.save(f, history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
