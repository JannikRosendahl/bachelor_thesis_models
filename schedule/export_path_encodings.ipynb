{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "cm = 1/2.54\n",
    "\n",
    "# force GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.src.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.src.layers import SimpleRNN, LSTM, Dense, Dropout, Bidirectional, Embedding, Flatten, Concatenate\n",
    "from keras import Sequential, Input\n",
    "from keras.losses import CategoricalCrossentropy, CategoricalFocalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/home/jrosendahl/datasets/cadets/sequences_export_benign_filetypes_path_ts/'\n",
    "autoencoder_path='/home/jrosendahl/sync/models/path_autoencoder/saves/path_autoencoder'\n",
    "export_file = 'path_encoding_map.pkl'\n",
    "\n",
    "\n",
    "encoder = load_model(os.path.join(autoencoder_path, 'encoder.keras'))\n",
    "\n",
    "with open(os.path.join(autoencoder_path, 'char_to_idx.json'), 'r') as f:\n",
    "    encoder_char_to_idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_length=50\n",
      "encoder_char_to_idx={'': 0, '.': 1, '/': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '_': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'j': 23, 'k': 24, 'l': 25, 'm': 26, 'n': 27, 'o': 28, 'p': 29, 'q': 30, 'r': 31, 's': 32, 't': 33, 'u': 34, 'v': 35, 'w': 36, 'x': 37, 'y': 38, 'z': 39}\n"
     ]
    }
   ],
   "source": [
    "print(f'{fixed_length=}')\n",
    "print(f'{encoder_char_to_idx=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 files (4.65%)\n",
      "processed 20000 files (9.30%)\n",
      "processed 30000 files (13.94%)\n",
      "processed 40000 files (18.59%)\n",
      "processed 50000 files (23.24%)\n",
      "processed 60000 files (27.89%)\n",
      "processed 70000 files (32.54%)\n",
      "processed 80000 files (37.18%)\n",
      "processed 90000 files (41.83%)\n",
      "processed 100000 files (46.48%)\n",
      "processed 110000 files (51.13%)\n",
      "processed 120000 files (55.78%)\n",
      "processed 130000 files (60.42%)\n",
      "processed 140000 files (65.07%)\n",
      "processed 150000 files (69.72%)\n",
      "processed 160000 files (74.37%)\n",
      "processed 170000 files (79.01%)\n",
      "processed 180000 files (83.66%)\n",
      "processed 190000 files (88.31%)\n",
      "processed 200000 files (92.96%)\n",
      "processed 210000 files (97.61%)\n",
      "data contains 35434964 paths\n",
      "unique paths: 242602\n"
     ]
    }
   ],
   "source": [
    "path_encoding_map = {}\n",
    "\n",
    "paths = []\n",
    "\n",
    "max_file_count = len(os.listdir(data_directory))\n",
    "file_count = 0\n",
    "for filename in os.listdir(data_directory):\n",
    "    file_count += 1\n",
    "    if file_count % 10000 == 0:\n",
    "        print(f'processed {file_count} files ({file_count/max_file_count*100:.2f}%)')\n",
    "    with open(os.path.join(data_directory, filename), 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n ').split(',')\n",
    "\n",
    "            for path in line[4:6]:\n",
    "                path_pp = preprocess_path(path)\n",
    "                paths.append(path_pp)\n",
    "\n",
    "print(f'data contains {len(paths)} paths')\n",
    "paths = list(set(paths))\n",
    "print(f'unique paths: {len(paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch 1/60\n",
      "processing batch 2/60\n",
      "processing batch 3/60\n",
      "processing batch 4/60\n",
      "processing batch 5/60\n",
      "processing batch 6/60\n",
      "processing batch 7/60\n",
      "processing batch 8/60\n",
      "processing batch 9/60\n",
      "processing batch 10/60\n",
      "processing batch 11/60\n",
      "processing batch 12/60\n",
      "processing batch 13/60\n",
      "processing batch 14/60\n",
      "processing batch 15/60\n",
      "processing batch 16/60\n",
      "processing batch 17/60\n",
      "processing batch 18/60\n",
      "processing batch 19/60\n",
      "processing batch 20/60\n",
      "processing batch 21/60\n",
      "processing batch 22/60\n",
      "processing batch 23/60\n",
      "processing batch 24/60\n",
      "processing batch 25/60\n",
      "processing batch 26/60\n",
      "processing batch 27/60\n",
      "processing batch 28/60\n",
      "processing batch 29/60\n",
      "processing batch 30/60\n",
      "processing batch 31/60\n",
      "processing batch 32/60\n",
      "processing batch 33/60\n",
      "processing batch 34/60\n",
      "processing batch 35/60\n",
      "processing batch 36/60\n",
      "processing batch 37/60\n",
      "processing batch 38/60\n",
      "processing batch 39/60\n",
      "processing batch 40/60\n",
      "processing batch 41/60\n",
      "processing batch 42/60\n",
      "processing batch 43/60\n",
      "processing batch 44/60\n",
      "processing batch 45/60\n",
      "processing batch 46/60\n",
      "processing batch 47/60\n",
      "processing batch 48/60\n",
      "processing batch 49/60\n",
      "processing batch 50/60\n",
      "processing batch 51/60\n",
      "processing batch 52/60\n",
      "processing batch 53/60\n",
      "processing batch 54/60\n",
      "processing batch 55/60\n",
      "processing batch 56/60\n",
      "processing batch 57/60\n",
      "processing batch 58/60\n",
      "processing batch 59/60\n",
      "processing batch 60/60\n",
      "encoded 242602 paths\n"
     ]
    }
   ],
   "source": [
    "# path_encoding_map[path_pp] = encoder.predict(np.array([vectorize_datapoint(path, encoder_char_to_idx, fixed_length)]), verbose=0)[0]\n",
    "\n",
    "# batch inference\n",
    "batch_size = 4096\n",
    "batches = len(paths) // batch_size\n",
    "if len(paths) % batch_size > 0:\n",
    "    batches += 1\n",
    "\n",
    "for i in range(batches):\n",
    "    print(f'processing batch {i+1}/{batches}')\n",
    "    batch = paths[i*batch_size:(i+1)*batch_size]\n",
    "    batch = [vectorize_datapoint(path, encoder_char_to_idx, fixed_length) for path in batch]\n",
    "    batch = np.array(batch)\n",
    "    batch = encoder.predict(batch, verbose=0)\n",
    "    for j in range(len(batch)):\n",
    "        path_encoding_map[paths[i*batch_size+j]] = batch[j]\n",
    "\n",
    "print(f'encoded {len(path_encoding_map)} paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write map to disk\n",
    "with open(export_file, 'wb') as f:\n",
    "    pickle.dump(path_encoding_map, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
